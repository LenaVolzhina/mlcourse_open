{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Статья](https://habrahabr.ru/company/ods/blog/323890/)\n",
    "Линейные модели классификации и регрессии\n",
    "\n",
    "План:\n",
    "* Линейная регрессия\n",
    " * Метод наименьших квадратов\n",
    " * Метод максимального правдоподобия\n",
    " * Разложение ошибки на смещение и разброс (Bias-variance decomposition)\n",
    " * Регуляризация линейной регрессии\n",
    "* Логистическая регрессия\n",
    " * Линейный классификатор\n",
    " * Логистическая регрессия как линейный классификатор\n",
    " * Принцип максимального правдоподобия и логистическая регрессия\n",
    " * L2-регуляризация логистической функции потерь\n",
    "* Наглядный пример регуляризации логистической регрессии\n",
    "* Где логистическая регрессия хороша и где не очень\n",
    " * Анализ отзывов IMDB к фильмам\n",
    " * XOR-проблема\n",
    "* Кривые валидации и обучения\n",
    "* Плюсы и минусы линейных моделей в задачах машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель:\n",
    "$y_i = \\sum_{j=0}^m w_j X_{ij} + \\epsilon_i$\n",
    "\n",
    "Ограничения: м.о. случаных ошибок = 0, дисперсия одинакова и конечна, случайные ошибки не скоррелированы\n",
    "\n",
    "### Метод наименьших квадратов\n",
    "$\\vec{w} = \\left(X^T X\\right)^{-1} X^T \\vec{y}$\n",
    "\n",
    "МНК даёт лучшую (с наименьшей дисперсией) оценку среди линейных и не смещенных\n",
    "\n",
    "### Метод наибольшего правдоподобия\n",
    "в предположении, что ошибки берутся из нормального центрированного распределения, записываем правдоподобие\n",
    "$\\begin{array}{rcl} p\\left(y_i \\mid X, \\vec{w}\\right) &=& \\sum_{j=1}^m w_j X_{ij} + \\mathcal{N}\\left(0, \\sigma^2\\right) \\\\ &=& \\mathcal{N}\\left(\\sum_{j=1}^m w_j X_{ij}, \\sigma^2\\right) \\end{array}$\n",
    "\n",
    "Максимизируя логарифм правдоподобия получим то же оптимальное $\\vec{w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-variance decomposition\n",
    "$ \\begin{array}{rcl} \\text{Err}\\left(\\vec{x}\\right) &=& \\mathbb{E}\\left[\\left(y - \\hat{f}\\left(\\vec{x}\\right)\\right)^2\\right] \\\\ &=& \\sigma^2 + f^2 + \\text{Var}\\left(\\hat{f}\\right) + \\mathbb{E}\\left[\\hat{f}\\right]^2 - 2f\\mathbb{E}\\left[\\hat{f}\\right] \\\\ &=& \\left(f - \\mathbb{E}\\left[\\hat{f}\\right]\\right)^2 + \\text{Var}\\left(\\hat{f}\\right) + \\sigma^2 \\\\ &=& \\text{Bias}\\left(\\hat{f}\\right)^2 + \\text{Var}\\left(\\hat{f}\\right) + \\sigma^2 \\end{array}$\n",
    "\n",
    "Ошибка прогноза любой модели вида $y = f\\left(\\vec{x}\\right) + \\epsilon$ складывается из:\n",
    "* квадрата смещения: $\\text{Bias}\\left(\\hat{f}\\right)^2 $ – средняя ошибка по всевозможным наборам данных;\n",
    "* дисперсии: $\\text{Var}\\left(\\hat{f}\\right)$ – вариативность ошибки, то, на сколько ошибка будет отличаться, если обучать модель на разных наборах данных;\n",
    "* неустранимой ошибки: $\\sigma^2$.\n",
    "\n",
    "![](img/biasvariance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регуляризация\n",
    "Приведение матрицы X к полному столбцовому рангу\n",
    "\n",
    "Пример: регуляризация Тихонова -- добавление к минимизации $L^2$ -- нормы от вектора весов. Имеет точное решение, называется гребневой регрессией (ridge regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия\n",
    "Идея: линейный классификатор -- делим пространство гиперплоскостью.\n",
    "\n",
    "Логистическая регрессия -- частный случай с удобной особенностью -- предсказывает вероятность\n",
    "\n",
    "$p_+(x_i) = P\\left(y_i = 1 \\mid \\vec{x_i}, \\vec{w}\\right) = \\sigma(\\vec{w}^T\\vec{x_i})$\n",
    "\n",
    "\n",
    "Обучается с помощью метода максимального правдоподобия. Запишем:\n",
    "\n",
    "$P\\left(y = y_i \\mid \\vec{x_i}, \\vec{w}\\right) = \\sigma(y_i\\vec{w}^T\\vec{x_i})$\n",
    "\n",
    "Выражение $M(\\vec{x_i}) = y_i\\vec{w}^T\\vec{x_i}$ называется отступом. Он положителен в случае верной классификации. Чем он больше по модулю, тем объек дальше от разделяющей гиперплоскости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбор для дз\n",
    "[отличная статья про softmax и его производную](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)\n",
    "\n",
    "Софтмакс оптимален с точки зрения максимального правдоподобия для логистической регрессии. Мы будем вычислять его от результатов векторного умножения X и w (w -- матрица размерности (число классов) x (число признаков))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
