{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению. Сессия №3\n",
    "<center>Автор материала: программист-исследователь Mail.Ru Group Юрий Кашницкий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашнее задание № 8\n",
    "## <center> Vowpal Wabbit в задаче классификации тегов вопросов на Stackoverflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## План\n",
    "    1. Введение\n",
    "    2. Описание данных\n",
    "    3. Предобработка данных\n",
    "    4. Обучение и проверка моделей\n",
    "    5. Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Введение\n",
    "\n",
    "В этом задании вы будете делать примерно то же, что я каждую неделю –  в Mail.Ru Group: обучать модели на выборке в несколько гигабайт. Задание можно выполнить и на Windows с Python, но я рекомендую поработать под \\*NIX-системой (например, через Docker) и активно использовать язык bash.\n",
    "Немного снобизма (простите, но правда): если вы захотите работать в лучших компаниях мира в области ML, вам все равно понадобится опыт работы с bash под UNIX.\n",
    "\n",
    "[Веб-форма](https://docs.google.com/forms/d/1VaxYXnmbpeP185qPk2_V_BzbeduVUVyTdLPQwSCxDGA/edit) для ответов.\n",
    "\n",
    "Для выполнения задания понадобится установленный Vowpal Wabbit (уже есть в докер-контейнере курса, см. инструкцию в Wiki [репозитория](https://github.com/Yorko/mlcourse_open) нашего курса) и примерно 70 Гб дискового пространства. Я тестировал решение не на каком-то суперкомпе, а на Macbook Pro 2015 (8 ядер, 16 Гб памяти), и самая тяжеловесная модель обучалась около 12 минут, так что задание реально выполнить и с простым железом. Но если вы планируете когда-либо арендовать сервера Amazon, можно попробовать это сделать уже сейчас.\n",
    "\n",
    "Материалы в помощь:\n",
    " - интерактивный [тьюториал](https://www.codecademy.com/en/courses/learn-the-command-line/lessons/environment/exercises/bash-profile) CodeAcademy по утилитам командной строки UNIX (примерно на час-полтора)\n",
    " - [статья](https://habrahabr.ru/post/280562/) про то, как арендовать на Amazon машину (еще раз: это не обязательно для выполнения задания, но будет хорошим опытом, если вы это делаете впервые)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Описание данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеются 10 Гб вопросов со StackOverflow – [скачайте](https://drive.google.com/file/d/1ZU4J3KhJDrHVMj48fROFcTsTZKorPGlG/view) и распакуйте архив. \n",
    "\n",
    "Формат данных простой:<br>\n",
    "<center>*текст вопроса* (слова через пробел) TAB *теги вопроса* (через пробел)\n",
    "\n",
    "Здесь TAB – это символ табуляции.\n",
    "Пример первой записи в выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: cannot open `hw8_data/stackoverflow.10kk.tsv' for reading: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!head -1 hw8_data/stackoverflow.10kk.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь у нас текст вопроса, затем табуляция и теги вопроса: *css, css3* и *css-selectors*. Всего в выборке таких вопросов 10 миллионов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000 hw8_data/stackoverflow.10kk.tsv\n",
      "CPU times: user 108 ms, sys: 32 ms, total: 140 ms\n",
      "Wall time: 5.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!wc -l hw8_data/stackoverflow.10kk.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на то, что такие данные я уже не хочу загружать в оперативную память и, пока можно, буду пользоваться эффективными утилитами UNIX –  head, tail, wc, cat, cut и прочими."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте выберем в наших данных все вопросы с тегами *javascript, java, python, ruby, php, c++, c#, go, scala* и  *swift* и подготовим обучающую выборку в формате Vowpal Wabbit. Будем решать задачу 10-классовой классификации вопросов по перечисленным тегам.\n",
    "\n",
    "Вообще, как мы видим, у каждого вопроса может быть несколько тегов, но мы упростим себе задачу и будем у каждого вопроса выбирать один из перечисленных тегов либо игнорировать вопрос, если таковых тегов нет. \n",
    "Но вообще VW поддерживает multilabel classification (аргумент  --multilabel_oaa).\n",
    "<br>\n",
    "<br>\n",
    "Реализуйте в виде отдельного файла `preprocess.py` код для подготовки данных. Он должен отобрать строки, в которых есть перечисленные теги, и переписать их в отдельный файл в формат Vowpal Wabbit. Детали:\n",
    " - скрипт должен работать с аргументами командной строки: с путями к файлам на входе и на выходе\n",
    " - строки обрабатываются по одной (можно использовать tqdm для подсчета числа итераций)\n",
    " - если табуляций в строке нет или их больше одной, считаем строку поврежденной и пропускаем\n",
    " - в противном случае смотрим, сколько в строке тегов из списка *javascript, java, python, ruby, php, c++, c#, go, scala* и  *swift*. Если ровно один, то записываем строку в выходной файл в формате VW: `label | text`, где `label` – число от 1 до 10 (1 - *javascript*, ... 10 – *swift*). Пропускаем те строки, где интересующих тегов больше или меньше одного \n",
    " - из текста вопроса надо выкинуть двоеточия и вертикальные палки, если они есть – в VW это спецсимволы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Должно получиться вот такое число строк – 4389054. 10 Гб у меня обработались примерно за 2 минуты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hw8_preprocess.py', 'hw8_data/stackoverflow.10kk.tsv', 'hw8_data/stackoverflow.vw']\n",
      "[2018-04-09 23:32:27.208934] Labels: {'javascript': 1, 'java': 2, 'python': 3, 'ruby': 4, 'php': 5, 'c++': 6, 'c#': 7, 'go': 8, 'scala': 9, 'swift': 10}\n",
      "[2018-04-09 23:33:01.102538] line #3398558: ' \tpython django\n",
      "'\n",
      "[2018-04-09 23:33:05.576487] line #3890296: ' \truby-on-rails ruby bundle gemfile\n",
      "'\n",
      "[2018-04-09 23:33:09.769830] line #4350090: ' \tc++ arrays\n",
      "'\n",
      "[2018-04-09 23:33:24.681299] line #5987980: ' \tc# vb.net excel csv export-to-csv\n",
      "'\n",
      "[2018-04-09 23:34:10.397366] corrupted: 15, bad tags: 5610931, ok lines: 4389054\n"
     ]
    }
   ],
   "source": [
    "# !python3 hw8_preprocess.py hw8_data/stackoverflow.10kk.tsv hw8_data/stackoverflow.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4389054 hw8_data/stackoverflow.vw\n",
      "CPU times: user 24 ms, sys: 8 ms, total: 32 ms\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!wc -l hw8_data/stackoverflow.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !gzip hw8_data/stackoverflow.10kk.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поделите выборку на обучающую, проверочную и тестовую части в равной пропорции - по  1463018 в каждый файл. Перемешивать не надо, первые 1463018 строк должны пойти в обучающую часть `stackoverflow_train.vw`, последние 1463018 – в тестовую `stackoverflow_test.vw`, оставшиеся – в проверочную `stackoverflow_valid.vw`. \n",
    "\n",
    "Также сохраните векторы ответов для проверочной и тестовой выборки в отдельные файлы `stackoverflow_valid_labels.txt` и `stackoverflow_test_labels.txt`.\n",
    "\n",
    "Тут вам помогут утилиты `head`, `tail`, `split`, `cat` и `cut`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head -1463018 hw8_data/stackoverflow.vw > hw8_data/stackoverflow_train.vw\n",
    "\n",
    "!head -2926036 hw8_data/stackoverflow.vw > hw8_data/tmp_stackoverflow_test.vw\n",
    "!tail -1463018 hw8_data/tmp_stackoverflow_test.vw > hw8_data/stackoverflow_test.vw\n",
    "!rm hw8_data/tmp_stackoverflow_test.vw\n",
    "\n",
    "!tail -1463018 hw8_data/stackoverflow.vw > hw8_data/stackoverflow_valid.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cut -c1 hw8_data/stackoverflow_test.vw > hw8_data/stackoverflow_test_labels.txt\n",
    "!cut -c1 hw8_data/stackoverflow_valid.vw > hw8_data/stackoverflow_valid_labels.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1463018 hw8_data/stackoverflow_train.vw\n",
      "1463018 hw8_data/stackoverflow_test.vw\n",
      "1463018 hw8_data/stackoverflow_valid.vw\n",
      "1 | i ve got some code in window scroll that checks if an element is visible then triggers another function however only the first section of code is firing both bits of code work in and of themselves if i swap their order whichever is on top fires correctly my code is as follows fn isonscreen function use strict var win window viewport top win scrolltop left win scrollleft bounds this offset viewport right viewport left + win width viewport bottom viewport top + win height bounds right bounds left + this outerwidth bounds bottom bounds top + this outerheight return viewport right lt bounds left viewport left gt bounds right viewport bottom lt bounds top viewport top gt bounds bottom window scroll function use strict var load_more_results ajax load_more_results isonscreen if load_more_results true loadmoreresults var load_more_staff ajax load_more_staff isonscreen if load_more_staff true loadmorestaff what am i doing wrong can you only fire one event from window scroll i assume not \n",
      "2 | i have used nssm to create a windows service for my selenium server instance v 2 48 2 of the selenium server standalone jar i have also set the service to log on as a local system account and have allowed the service to interact with the desktop when i have used a particular account for the service instead of the local system account internet explorer would not launch i noticed that after a few days the hard disk would start filling up with temporary internet files at the following location c windows syswow64 config systemprofile appdata local microsoft windows temporary internet files content ie5 after a few days i saw that the size of this folder was 30 gb i have had to manually clear out this folder i ve used the following command to create the service nssm install seleniumhub java jar c selenium server selenium server standalone 2 48 2 jar dwebdriver chrome driver c selenium server chromedriver exe dwebdriver ie driver c selenium server iedriverserver exe has anyone else run into this issue \n",
      "9 | i ve been studying about how i could develop a distributed architecture that implements the protocol request response using the concept of concurrency through actors i concluded that the best way to do this is by creating a response system with synchronous handling of futures promises and soon after the response leaving an open channel to receive notifications thus an architecture that would work exactly like a inbox message it has some problems thus i would have to maintain two endpoints actors in the two layers the problem the view module requests that a particular element is processed she sends this command to be processed via remoteactor on the application server this server should immediately return the promise that it will notify you when the element is processed after this the view module will be waiting the notification of completion of processing how do you see this problem i am using scala akka and google guice i believe it is a generic problem that everyone can make use of their solutions excuse me if i m hurting the terms of stackoverflow site thanks in advance \n",
      "1463018 hw8_data/stackoverflow_test_labels.txt\n",
      "1463018 hw8_data/stackoverflow_valid_labels.txt\n",
      "2\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "!wc -l hw8_data/stackoverflow_train.vw\n",
    "!wc -l hw8_data/stackoverflow_test.vw\n",
    "!wc -l hw8_data/stackoverflow_valid.vw\n",
    "\n",
    "!head -1 hw8_data/stackoverflow_train.vw\n",
    "!head -1 hw8_data/stackoverflow_test.vw\n",
    "!head -1 hw8_data/stackoverflow_valid.vw\n",
    "\n",
    "!wc -l hw8_data/stackoverflow_test_labels.txt\n",
    "!wc -l hw8_data/stackoverflow_valid_labels.txt\n",
    "\n",
    "!head -1 hw8_data/stackoverflow_test_labels.txt\n",
    "!head -1 hw8_data/stackoverflow_valid_labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Обучение и проверка моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите Vowpal Wabbit на выборке `stackoverflow_train.vw` 9 раз, перебирая параметры passes (1,3,5), ngram (1,2,3).\n",
    "Остальные параметры укажите следующие: `loss_function=hinge`, `bit_precision`=28 и `seed`=17. Также скажите VW, что это 10-классовая задача.\n",
    "\n",
    "Проверяйте долю правильных ответов на выборке `stackoverflow_valid.vw`. Выберите лучшую модель и проверьте качество на выборке `stackoverflow_test.vw`.\n",
    "\n",
    "[HABR](https://habrahabr.ru/company/ods/blog/326418/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!vw --oaa 10 -d hw8_data/stackoverflow_train.vw -f hw8_data/models/passes_1_ngram_1_model.vw  \\\n",
      "    --loss_function hinge --bit_precision 28 --random_seed 17 --cache_file cache.vw \\\n",
      "    --passes 1 --ngram 1\n",
      "\n",
      "!vw --oaa 10 -d hw8_data/stackoverflow_train.vw -f hw8_data/models/passes_1_ngram_2_model.vw  \\\n",
      "    --loss_function hinge --bit_precision 28 --random_seed 17 --cache_file cache.vw \\\n",
      "    --passes 1 --ngram 2\n",
      "\n",
      "!vw --oaa 10 -d hw8_data/stackoverflow_train.vw -f hw8_data/models/passes_1_ngram_3_model.vw  \\\n",
      "    --loss_function hinge --bit_precision 28 --random_seed 17 --cache_file cache.vw \\\n",
      "    --passes 1 --ngram 3\n",
      "\n",
      "!vw --oaa 10 -d hw8_data/stackoverflow_train.vw -f hw8_data/models/passes_3_ngram_1_model.vw  \\\n",
      "    --loss_function hinge --bit_precision 28 --random_seed 17 --cache_file cache.vw \\\n",
      "    --passes 3 --ngram 1\n",
      "\n",
      "!vw --oaa 10 -d hw8_data/stackoverflow_train.vw -f hw8_data/models/passes_3_ngram_2_model.vw  \\\n",
      "    --loss_function hinge --bit_precision 28 --random_seed 17 --cache_file cache.vw \\\n",
      "    --passes 3 --ngram 2\n",
      "\n",
      "!vw --oaa 10 -d hw8_data/stackoverflow_train.vw -f hw8_data/models/passes_3_ngram_3_model.vw  \\\n",
      "    --loss_function hinge --bit_precision 28 --random_seed 17 --cache_file cache.vw \\\n",
      "    --passes 3 --ngram 3\n",
      "\n",
      "!vw --oaa 10 -d hw8_data/stackoverflow_train.vw -f hw8_data/models/passes_5_ngram_1_model.vw  \\\n",
      "    --loss_function hinge --bit_precision 28 --random_seed 17 --cache_file cache.vw \\\n",
      "    --passes 5 --ngram 1\n",
      "\n",
      "!vw --oaa 10 -d hw8_data/stackoverflow_train.vw -f hw8_data/models/passes_5_ngram_2_model.vw  \\\n",
      "    --loss_function hinge --bit_precision 28 --random_seed 17 --cache_file cache.vw \\\n",
      "    --passes 5 --ngram 2\n",
      "\n",
      "!vw --oaa 10 -d hw8_data/stackoverflow_train.vw -f hw8_data/models/passes_5_ngram_3_model.vw  \\\n",
      "    --loss_function hinge --bit_precision 28 --random_seed 17 --cache_file cache.vw \\\n",
      "    --passes 5 --ngram 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run models learning\n",
    "params = {'passes': [1, 3, 5], 'ngram': [1, 2, 3]}\n",
    "models = []\n",
    "for passes in params['passes']:\n",
    "    for ngram in params['ngram']:\n",
    "        model_name = \"passes_{}_ngram_{}\".format(passes, ngram)\n",
    "        print(\n",
    "            '!vw --oaa 10 -d hw8_data/stackoverflow_train.vw -f hw8_data/models/{}_model.vw  \\\\\\n'\n",
    "            '    --loss_function hinge --bit_precision 28 --random_seed 17 --cache_file cache.vw \\\\\\n'\n",
    "            '    --passes {} --ngram {}\\n'.format(model_name, passes, ngram)\n",
    "        )\n",
    "        models.append(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!vw -i hw8_data/models/passes_1_ngram_1_model.vw -t -d hw8_data/stackoverflow_valid.vw \\\n",
      "    -p hw8_data/models/passes_1_ngram_1_valid_answers.tsv\n",
      "\n",
      "!vw -i hw8_data/models/passes_1_ngram_2_model.vw -t -d hw8_data/stackoverflow_valid.vw \\\n",
      "    -p hw8_data/models/passes_1_ngram_2_valid_answers.tsv\n",
      "\n",
      "!vw -i hw8_data/models/passes_1_ngram_3_model.vw -t -d hw8_data/stackoverflow_valid.vw \\\n",
      "    -p hw8_data/models/passes_1_ngram_3_valid_answers.tsv\n",
      "\n",
      "!vw -i hw8_data/models/passes_3_ngram_1_model.vw -t -d hw8_data/stackoverflow_valid.vw \\\n",
      "    -p hw8_data/models/passes_3_ngram_1_valid_answers.tsv\n",
      "\n",
      "!vw -i hw8_data/models/passes_3_ngram_2_model.vw -t -d hw8_data/stackoverflow_valid.vw \\\n",
      "    -p hw8_data/models/passes_3_ngram_2_valid_answers.tsv\n",
      "\n",
      "!vw -i hw8_data/models/passes_3_ngram_3_model.vw -t -d hw8_data/stackoverflow_valid.vw \\\n",
      "    -p hw8_data/models/passes_3_ngram_3_valid_answers.tsv\n",
      "\n",
      "!vw -i hw8_data/models/passes_5_ngram_1_model.vw -t -d hw8_data/stackoverflow_valid.vw \\\n",
      "    -p hw8_data/models/passes_5_ngram_1_valid_answers.tsv\n",
      "\n",
      "!vw -i hw8_data/models/passes_5_ngram_2_model.vw -t -d hw8_data/stackoverflow_valid.vw \\\n",
      "    -p hw8_data/models/passes_5_ngram_2_valid_answers.tsv\n",
      "\n",
      "!vw -i hw8_data/models/passes_5_ngram_3_model.vw -t -d hw8_data/stackoverflow_valid.vw \\\n",
      "    -p hw8_data/models/passes_5_ngram_3_valid_answers.tsv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run models applying\n",
    "for model_name in models:\n",
    "    print(\n",
    "        '!vw -i hw8_data/models/{}_model.vw -t -d hw8_data/stackoverflow_valid.vw \\\\\\n'\n",
    "        '    -p hw8_data/models/{}_valid_answers.tsv\\n'.format(model_name, model_name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passes_1_ngram_1 0.896461900306\n",
      "passes_1_ngram_2 0.911553317562\n",
      "passes_1_ngram_3 0.909102218224\n",
      "passes_3_ngram_1 0.897642337717\n",
      "passes_3_ngram_2 0.90948977353\n",
      "passes_3_ngram_3 0.906454265398\n",
      "passes_5_ngram_1 0.897426345695\n",
      "passes_5_ngram_2 0.910154154053\n",
      "passes_5_ngram_3 0.907330536829\n"
     ]
    }
   ],
   "source": [
    "# calculate metrics\n",
    "valid_true_answers = pd.read_csv('hw8_data/stackoverflow_valid_labels.txt')\n",
    "\n",
    "for model_name in models:\n",
    "    model_answers = pd.read_csv('hw8_data/models/{}_valid_answers.tsv'.format(model_name))\n",
    "    acc = accuracy_score(y_true=valid_true_answers, y_pred=model_answers)\n",
    "    print(model_name, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Вопрос 1.</font> Какое сочетание параметров дает наибольшую долю правильных ответов на проверочной выборке `stackoverflow_valid.vw`?\n",
    "- Биграммы и 3 прохода по выборке\n",
    "- Триграммы и 5 проходов по выборке\n",
    "- **Биграммы и 1 проход по выборке**\n",
    "- Униграммы и 1 проход по выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте лучшую (по доле правильных ответов на валидации) модель на тестовой выборке. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = hw8_data/models/passes_1_ngram_2_test_answers.tsv\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = hw8_data/stackoverflow_test.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        7      354\n",
      "0.500000 0.000000            2            2.0        7        7      146\n",
      "0.250000 0.000000            4            4.0        5        5      516\n",
      "0.125000 0.000000            8            8.0        7        7      286\n",
      "0.125000 0.125000           16           16.0        6        6      716\n",
      "0.062500 0.000000           32           32.0        2        2      798\n",
      "0.062500 0.062500           64           64.0        5        5     2120\n",
      "0.046875 0.031250          128          128.0        2        2      262\n",
      "0.074219 0.101562          256          256.0        6        6      174\n",
      "0.060547 0.046875          512          512.0        7        7       68\n",
      "0.056641 0.052734         1024         1024.0        1        1      174\n",
      "0.061523 0.066406         2048         2048.0        2        2      484\n",
      "0.062500 0.063477         4096         4096.0        2        2      524\n",
      "0.065918 0.069336         8192         8192.0        7        7      542\n",
      "0.067444 0.068970        16384        16384.0        2        2      252\n",
      "0.067230 0.067017        32768        32768.0        6        6      180\n",
      "0.068008 0.068787        65536        65536.0        5        5      570\n",
      "0.067581 0.067154       131072       131072.0        5        5      350\n",
      "0.067097 0.066612       262144       262144.0        2        2      494\n",
      "0.067423 0.067749       524288       524288.0        7        7       70\n",
      "0.067742 0.068062      1048576      1048576.0        1        1      218\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.067643\n",
      "total feature number = 580610428\n"
     ]
    }
   ],
   "source": [
    "''' ВАШ КОД ЗДЕСЬ '''\n",
    "!vw -i hw8_data/models/passes_1_ngram_2_model.vw -t -d hw8_data/stackoverflow_test.vw \\\n",
    "    -p hw8_data/models/passes_1_ngram_2_test_answers.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91140157633164887"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_true_answers = pd.read_csv('hw8_data/stackoverflow_test_labels.txt')\n",
    "model_answers = pd.read_csv('hw8_data/models/{}_test_answers.tsv'.format('passes_1_ngram_2'))\n",
    "accuracy_score(y_true=test_true_answers, y_pred=model_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Вопрос 2.</font> Как соотносятся доли правильных ответов лучшей (по доле правильных ответов на валидации) модели на проверочной и на тестовой выборках? (здесь % – это процентный пункт, т.е., скажем, снижение с 50% до 40% – это на 10%, а не 20%).\n",
    "- На тестовой ниже примерно на 2%\n",
    "- На тестовой ниже примерно на 3%\n",
    "- **Результаты почти одинаковы – отличаются меньше чем на 0.5%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите VW с параметрами, подобранными на проверочной выборке, теперь на объединении обучающей и проверочной выборок. Посчитайте долю правильных ответов на тестовой выборке. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat hw8_data/stackoverflow_train.vw hw8_data/stackoverflow_valid.vw > hw8_data/stackoverflow_train_valid.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = hw8_data/models/passes_1_ngram_2_ext_model.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "creating cache_file = cache1.vw\n",
      "Reading datafile = hw8_data/stackoverflow_train_valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      320\n",
      "0.500000 1.000000            2            2.0        4        1      134\n",
      "0.750000 1.000000            4            4.0        7        1      174\n",
      "0.750000 0.750000            8            8.0        7        1      188\n",
      "0.750000 0.750000           16           16.0        7        7      416\n",
      "0.781250 0.812500           32           32.0        7        2      346\n",
      "0.750000 0.718750           64           64.0        3        3      406\n",
      "0.648438 0.546875          128          128.0        1        7       56\n",
      "0.617188 0.585938          256          256.0        5        1      336\n",
      "0.548828 0.480469          512          512.0        2        2      604\n",
      "0.454102 0.359375         1024         1024.0        3        3      244\n",
      "0.375000 0.295898         2048         2048.0        1        5      164\n",
      "0.306396 0.237793         4096         4096.0        1        1      156\n",
      "0.254761 0.203125         8192         8192.0        2        2      222\n",
      "0.211426 0.168091        16384        16384.0        7        7      502\n",
      "0.176117 0.140808        32768        32768.0        4        5      266\n",
      "0.147873 0.119629        65536        65536.0        5        5      288\n",
      "0.126411 0.104950       131072       131072.0        7        2      508\n",
      "0.108402 0.090393       262144       262144.0        7        7      200\n",
      "0.097092 0.085781       524288       524288.0        1        1     1634\n",
      "0.086005 0.074919      1048576      1048576.0        1        1     1140\n",
      "0.077353 0.068702      2097152      2097152.0        5        5      170\n",
      "\n",
      "finished run\n",
      "number of examples = 2926036\n",
      "weighted example sum = 2926036.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.073513\n",
      "total feature number = 1163296978\n",
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = hw8_data/models/passes_1_ngram_2_ext_test_answers.tsv\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = hw8_data/stackoverflow_test.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        7      354\n",
      "0.500000 0.000000            2            2.0        7        7      146\n",
      "0.250000 0.000000            4            4.0        5        5      516\n",
      "0.125000 0.000000            8            8.0        7        7      286\n",
      "0.062500 0.000000           16           16.0        6        6      716\n",
      "0.031250 0.000000           32           32.0        2        2      798\n",
      "0.062500 0.093750           64           64.0        5        5     2120\n",
      "0.054688 0.046875          128          128.0        2        2      262\n",
      "0.074219 0.093750          256          256.0        6        6      174\n",
      "0.056641 0.039062          512          512.0        7        7       68\n",
      "0.054688 0.052734         1024         1024.0        1        1      174\n",
      "0.059082 0.063477         2048         2048.0        2        2      484\n",
      "0.055176 0.051270         4096         4096.0        2        2      524\n",
      "0.059937 0.064697         8192         8192.0        7        7      542\n",
      "0.063416 0.066895        16384        16384.0        2        2      252\n",
      "0.062836 0.062256        32768        32768.0        6        2      180\n",
      "0.062973 0.063110        65536        65536.0        5        5      570\n",
      "0.063011 0.063049       131072       131072.0        5        5      350\n",
      "0.062721 0.062431       262144       262144.0        2        2      494\n",
      "0.063368 0.064014       524288       524288.0        7        7       70\n",
      "0.063442 0.063517      1048576      1048576.0        1        1      218\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.063301\n",
      "total feature number = 580610428\n"
     ]
    }
   ],
   "source": [
    "''' ВАШ КОД ЗДЕСЬ '''\n",
    "# NB! cache file\n",
    "\n",
    "!vw --oaa 10 -d hw8_data/stackoverflow_train_valid.vw -f hw8_data/models/passes_1_ngram_2_ext_model.vw  \\\n",
    "    --bit_precision 28 --random_seed 17 --cache_file cache1.vw \\\n",
    "    --passes 1 --ngram 2\n",
    "# --loss_function hinge\n",
    "\n",
    "!vw -i hw8_data/models/passes_1_ngram_2_ext_model.vw -t -d hw8_data/stackoverflow_test.vw \\\n",
    "    -p hw8_data/models/passes_1_ngram_2_ext_test_answers.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91543843988142315"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_true_answers = pd.read_csv('hw8_data/stackoverflow_test_labels.txt')\n",
    "model_answers = pd.read_csv('hw8_data/models/{}_test_answers.tsv'.format('passes_1_ngram_2_ext'))\n",
    "accuracy_score(y_true=test_true_answers, y_pred=model_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003885122319423173"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.91543843988142315 - 0.911553317562"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Вопрос 3.</font> На сколько процентных пунктов повысилась доля правильных ответов модели после обучения на вдвое большей выборке (обучающая `stackoverflow_train.vw` + проверочная `stackoverflow_valid.vw`) по сравнению с моделью, обученной только на `stackoverflow_train.vw`?\n",
    " - 0.1%\n",
    " - **0.4%**\n",
    " - 0.8%\n",
    " - 1.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Повторим то же самое для не hinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!vw --oaa 10 -d hw8_data/stackoverflow_train.vw -f hw8_data/models/passes_1_ngram_1_squared_model.vw  \\\n",
      "    --bit_precision 28 --random_seed 17 --cache_file cache.vw \\\n",
      "    --passes 1 --ngram 1\n",
      "\n",
      "!vw --oaa 10 -d hw8_data/stackoverflow_train.vw -f hw8_data/models/passes_1_ngram_2_squared_model.vw  \\\n",
      "    --bit_precision 28 --random_seed 17 --cache_file cache.vw \\\n",
      "    --passes 1 --ngram 2\n",
      "\n",
      "!vw --oaa 10 -d hw8_data/stackoverflow_train.vw -f hw8_data/models/passes_1_ngram_3_squared_model.vw  \\\n",
      "    --bit_precision 28 --random_seed 17 --cache_file cache.vw \\\n",
      "    --passes 1 --ngram 3\n",
      "\n",
      "!vw --oaa 10 -d hw8_data/stackoverflow_train.vw -f hw8_data/models/passes_3_ngram_1_squared_model.vw  \\\n",
      "    --bit_precision 28 --random_seed 17 --cache_file cache.vw \\\n",
      "    --passes 3 --ngram 1\n",
      "\n",
      "!vw --oaa 10 -d hw8_data/stackoverflow_train.vw -f hw8_data/models/passes_3_ngram_2_squared_model.vw  \\\n",
      "    --bit_precision 28 --random_seed 17 --cache_file cache.vw \\\n",
      "    --passes 3 --ngram 2\n",
      "\n",
      "!vw --oaa 10 -d hw8_data/stackoverflow_train.vw -f hw8_data/models/passes_3_ngram_3_squared_model.vw  \\\n",
      "    --bit_precision 28 --random_seed 17 --cache_file cache.vw \\\n",
      "    --passes 3 --ngram 3\n",
      "\n",
      "!vw --oaa 10 -d hw8_data/stackoverflow_train.vw -f hw8_data/models/passes_5_ngram_1_squared_model.vw  \\\n",
      "    --bit_precision 28 --random_seed 17 --cache_file cache.vw \\\n",
      "    --passes 5 --ngram 1\n",
      "\n",
      "!vw --oaa 10 -d hw8_data/stackoverflow_train.vw -f hw8_data/models/passes_5_ngram_2_squared_model.vw  \\\n",
      "    --bit_precision 28 --random_seed 17 --cache_file cache.vw \\\n",
      "    --passes 5 --ngram 2\n",
      "\n",
      "!vw --oaa 10 -d hw8_data/stackoverflow_train.vw -f hw8_data/models/passes_5_ngram_3_squared_model.vw  \\\n",
      "    --bit_precision 28 --random_seed 17 --cache_file cache.vw \\\n",
      "    --passes 5 --ngram 3\n",
      "\n",
      "!vw -i hw8_data/models/passes_1_ngram_1_squared_model.vw -t -d hw8_data/stackoverflow_valid.vw \\\n",
      "    -p hw8_data/models/passes_1_ngram_1_squared_valid_answers.tsv\n",
      "\n",
      "!vw -i hw8_data/models/passes_1_ngram_2_squared_model.vw -t -d hw8_data/stackoverflow_valid.vw \\\n",
      "    -p hw8_data/models/passes_1_ngram_2_squared_valid_answers.tsv\n",
      "\n",
      "!vw -i hw8_data/models/passes_1_ngram_3_squared_model.vw -t -d hw8_data/stackoverflow_valid.vw \\\n",
      "    -p hw8_data/models/passes_1_ngram_3_squared_valid_answers.tsv\n",
      "\n",
      "!vw -i hw8_data/models/passes_3_ngram_1_squared_model.vw -t -d hw8_data/stackoverflow_valid.vw \\\n",
      "    -p hw8_data/models/passes_3_ngram_1_squared_valid_answers.tsv\n",
      "\n",
      "!vw -i hw8_data/models/passes_3_ngram_2_squared_model.vw -t -d hw8_data/stackoverflow_valid.vw \\\n",
      "    -p hw8_data/models/passes_3_ngram_2_squared_valid_answers.tsv\n",
      "\n",
      "!vw -i hw8_data/models/passes_3_ngram_3_squared_model.vw -t -d hw8_data/stackoverflow_valid.vw \\\n",
      "    -p hw8_data/models/passes_3_ngram_3_squared_valid_answers.tsv\n",
      "\n",
      "!vw -i hw8_data/models/passes_5_ngram_1_squared_model.vw -t -d hw8_data/stackoverflow_valid.vw \\\n",
      "    -p hw8_data/models/passes_5_ngram_1_squared_valid_answers.tsv\n",
      "\n",
      "!vw -i hw8_data/models/passes_5_ngram_2_squared_model.vw -t -d hw8_data/stackoverflow_valid.vw \\\n",
      "    -p hw8_data/models/passes_5_ngram_2_squared_valid_answers.tsv\n",
      "\n",
      "!vw -i hw8_data/models/passes_5_ngram_3_squared_model.vw -t -d hw8_data/stackoverflow_valid.vw \\\n",
      "    -p hw8_data/models/passes_5_ngram_3_squared_valid_answers.tsv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run models learning\n",
    "models_sq = []\n",
    "for passes in params['passes']:\n",
    "    for ngram in params['ngram']:\n",
    "        model_name = \"passes_{}_ngram_{}_squared\".format(passes, ngram)\n",
    "        print(\n",
    "            '!vw --oaa 10 -d hw8_data/stackoverflow_train.vw -f hw8_data/models/{}_model.vw  \\\\\\n'\n",
    "            '    --bit_precision 28 --random_seed 17 --cache_file cache.vw \\\\\\n'\n",
    "            '    --passes {} --ngram {}\\n'.format(model_name, passes, ngram)\n",
    "        )\n",
    "        models_sq.append(model_name)\n",
    "\n",
    "# run models applying\n",
    "for model_name in models_sq:\n",
    "    print(\n",
    "        '!vw -i hw8_data/models/{}_model.vw -t -d hw8_data/stackoverflow_valid.vw \\\\\\n'\n",
    "        '    -p hw8_data/models/{}_valid_answers.tsv\\n'.format(model_name, model_name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passes_1_ngram_1_squared 0.894791379731\n",
      "passes_1_ngram_2_squared 0.910434396866\n",
      "passes_1_ngram_3_squared 0.907942286385\n",
      "passes_3_ngram_1_squared 0.893807112289\n",
      "passes_3_ngram_2_squared 0.906871895542\n",
      "passes_3_ngram_3_squared 0.905299118192\n",
      "passes_5_ngram_1_squared 0.893072329303\n",
      "passes_5_ngram_2_squared 0.908384523215\n",
      "passes_5_ngram_3_squared 0.905296384116\n"
     ]
    }
   ],
   "source": [
    "# calculate metrics\n",
    "valid_true_answers = pd.read_csv('hw8_data/stackoverflow_valid_labels.txt')\n",
    "\n",
    "for model_name in models_sq:\n",
    "    model_answers = pd.read_csv('hw8_data/models/{}_valid_answers.tsv'.format(model_name))\n",
    "    acc = accuracy_score(y_true=valid_true_answers, y_pred=model_answers)\n",
    "    print(model_name, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = hw8_data/models/passes_1_ngram_2_squared_ext_model.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using cache_file = cache1.vw\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      320\n",
      "0.500000 1.000000            2            2.0        4        1      134\n",
      "0.750000 1.000000            4            4.0        7        1      174\n",
      "0.750000 0.750000            8            8.0        7        1      188\n",
      "0.750000 0.750000           16           16.0        7        7      416\n",
      "0.781250 0.812500           32           32.0        7        2      346\n",
      "0.765625 0.750000           64           64.0        3        3      406\n",
      "0.664062 0.562500          128          128.0        1        7       56\n",
      "0.597656 0.531250          256          256.0        5        3      336\n",
      "0.527344 0.457031          512          512.0        2        2      604\n",
      "0.429688 0.332031         1024         1024.0        3        3      244\n",
      "0.362793 0.295898         2048         2048.0        1        5      164\n",
      "0.298096 0.233398         4096         4096.0        1        1      156\n",
      "0.249878 0.201660         8192         8192.0        2        2      222\n",
      "0.210266 0.170654        16384        16384.0        7        7      502\n",
      "0.175262 0.140259        32768        32768.0        4        5      266\n",
      "0.148163 0.121063        65536        65536.0        5        5      288\n",
      "0.127380 0.106598       131072       131072.0        7        2      508\n",
      "0.109459 0.091537       262144       262144.0        7        7      200\n",
      "0.097469 0.085480       524288       524288.0        1        1     1634\n",
      "0.087222 0.076975      1048576      1048576.0        1        1     1140\n",
      "0.078821 0.070420      2097152      2097152.0        5        5      170\n",
      "\n",
      "finished run\n",
      "number of examples = 2926036\n",
      "weighted example sum = 2926036.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.074969\n",
      "total feature number = 1163296978\n",
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = hw8_data/models/passes_1_ngram_2_squared_ext_test_answers.tsv\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = hw8_data/stackoverflow_test.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        7      354\n",
      "0.500000 0.000000            2            2.0        7        7      146\n",
      "0.250000 0.000000            4            4.0        5        5      516\n",
      "0.125000 0.000000            8            8.0        7        7      286\n",
      "0.062500 0.000000           16           16.0        6        6      716\n",
      "0.031250 0.000000           32           32.0        2        2      798\n",
      "0.046875 0.062500           64           64.0        5        5     2120\n",
      "0.046875 0.046875          128          128.0        2        2      262\n",
      "0.070312 0.093750          256          256.0        6        6      174\n",
      "0.054688 0.039062          512          512.0        7        7       68\n",
      "0.054688 0.054688         1024         1024.0        1        1      174\n",
      "0.057617 0.060547         2048         2048.0        2        2      484\n",
      "0.056152 0.054688         4096         4096.0        2        2      524\n",
      "0.059448 0.062744         8192         8192.0        7        7      542\n",
      "0.063660 0.067871        16384        16384.0        2        2      252\n",
      "0.063629 0.063599        32768        32768.0        6        2      180\n",
      "0.064407 0.065186        65536        65536.0        5        5      570\n",
      "0.064339 0.064270       131072       131072.0        5        5      350\n",
      "0.064377 0.064415       262144       262144.0        2        2      494\n",
      "0.064564 0.064751       524288       524288.0        7        7       70\n",
      "0.064672 0.064779      1048576      1048576.0        1        1      218\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.064467\n",
      "total feature number = 580610428\n"
     ]
    }
   ],
   "source": [
    "''' ВАШ КОД ЗДЕСЬ '''\n",
    "# NB! cache file\n",
    "\n",
    "!vw --oaa 10 -d hw8_data/stackoverflow_train_valid.vw -f hw8_data/models/passes_1_ngram_2_squared_ext_model.vw  \\\n",
    "    --bit_precision 28 --random_seed 17 --cache_file cache1.vw \\\n",
    "    --passes 1 --ngram 2\n",
    "# --loss_function hinge\n",
    "\n",
    "!vw -i hw8_data/models/passes_1_ngram_2_squared_ext_model.vw -t -d hw8_data/stackoverflow_test.vw \\\n",
    "    -p hw8_data/models/passes_1_ngram_2_squared_ext_test_answers.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91439607331972217"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_true_answers = pd.read_csv('hw8_data/stackoverflow_test_labels.txt')\n",
    "model_answers = pd.read_csv('hw8_data/models/{}_test_answers.tsv'.format('passes_1_ngram_2_squared_ext'))\n",
    "accuracy_score(y_true=test_true_answers, y_pred=model_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0039616764537221405"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.91439607331972217 - 0.910434396866"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании мы только познакомились с Vowpal Wabbit. Что еще можно попробовать:\n",
    " - Классификация с несколькими ответами (multilabel classification, аргумент  `multilabel_oaa`) – формат данных тут как раз под такую задачу\n",
    " - Настройка параметров VW с hyperopt, авторы библиотеки утверждают, что качество должно сильно зависеть от параметров изменения шага градиентного спуска (`initial_t` и `power_t`). Также можно потестировать разные функции потерь – обучать логистическую регресиию или линейный SVM\n",
    " - Познакомиться с факторизационными машинами и их реализацией в VW (аргумент `lrq`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
